{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyspark1\n",
    "Создаем простой DataFrame и пробуем на нем функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несколько полезных ссылок, прежде, чем начнем:  \n",
    "- Jupyter Notebook:  \n",
    "http://83.166.236.17:8000/user/v.alehin/notebooks/\n",
    "- Yarn:  \n",
    "http://91.219.226.252:8088/cluster\n",
    "http://91.219.226.252:8088/cluster/scheduler\n",
    "- App info:  \n",
    "http://91.219.226.252:8088/cluster/app/{application_id}/\n",
    "- Spark:       \n",
    "http://91.219.226.252:8088/proxy/{application_id}/stages/\n",
    "- Luigi visualizer:  \n",
    "http://83.166.236.17:8082/static/visualiser/index.html\n",
    "- Airflow:  \n",
    "http://83.166.236.17:8080/admin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark1\n",
    "# Создаем простой DataFrame и пробуем на нем функции\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"HADOOP_CONF_DIR\"]=\"/etc/hadoop/conf\"\n",
    "# os.environ[\"SPARK_HOME\"]=\"/usr/hdp/current/spark2-client\"\n",
    "# os.environ[\"JAVA_HOME\"]=\"/usr/java/jdk1.8.0_191/jre\"\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"yarn\")\\\n",
    "    .appName(\"v_alehin_pyspark1\")\\\n",
    "    .config(\"spark.executor.instances\", \"1\")\\\n",
    "    .config(\"spark.executor.memory\", \"1G\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"300s\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"1000\")\\\n",
    "    .config(\"spark.driver.memory\", \"1G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1G\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark_context = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Urls:\n",
    "Yarn       http://91.219.226.252:8088/cluster/scheduler\n",
    "Spark      http://91.219.226.252:8088/proxy/{app_id}/stages/\n",
    "App info   http://91.219.226.252:8088/cluster/app/{app_id}/\n",
    "\"\"\".format(app_id=spark_context.applicationId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [1, \"MaiL\", 100.0],\n",
    "    [2, \"GooGle\", 93.4],\n",
    "    [3, \"YaNDEX\", 59.9],\n",
    "    [4, \"sPUtnik\", 1.33],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "dfSchema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"company_name\", StringType()),\n",
    "    StructField(\"score\", DoubleType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, schema=dfSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data, schema=dfSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "resultDF = (df\n",
    "            .select(\n",
    "                sf.col(\"id\"),\n",
    "                sf.lower(sf.col(\"company_name\")).alias(\"company_name\"),\n",
    "                (sf.col(\"score\") / 100).alias(\"score\")\n",
    "            )\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно добавлять колонки\n",
    "(\n",
    "    resultDF\n",
    "    .withColumn(\"company_name_upper\", sf.upper(sf.col(\"company_name\")))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Можно перезаписывать существующие колонки\n",
    "(\n",
    "    resultDF\n",
    "    .withColumn(\"company_name\", sf.upper(sf.col(\"company_name\")))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# После работы обязательно отключаем спарк и отдаем ресурсы!\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
