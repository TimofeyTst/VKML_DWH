{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyspark2\n",
    "Читаем данные с hdfs и делаем джоин двух таблиц.   \n",
    "Результат сохраняем в один файл с сортировкой по возрастанию цены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"HADOOP_CONF_DIR\"]=\"/etc/hadoop/conf\"\n",
    "# os.environ[\"SPARK_HOME\"]=\"/usr/hdp/current/spark2-client\"\n",
    "# os.environ[\"JAVA_HOME\"]=\"/usr/java/jdk1.8.0_191/jre\"\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"yarn\")\\\n",
    "    .appName(\"v_alehin_pyspark2\")\\\n",
    "    .config(\"spark.executor.instances\", \"1\")\\\n",
    "    .config(\"spark.executor.memory\", \"1G\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"300s\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"1000\")\\\n",
    "    .config(\"spark.driver.memory\", \"1G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1G\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark_context = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Urls:\n",
    "Yarn       http://91.219.226.252:8088/cluster/scheduler\n",
    "Spark      http://91.219.226.252:8088/proxy/{app_id}/stages/\n",
    "App info   http://91.219.226.252:8088/cluster/app/{app_id}/\n",
    "\"\"\".format(app_id=spark_context.applicationId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "user = \"\"  # your login\n",
    "\n",
    "product_path = \"/user/{}/data/data1/shop_product.csv\".format(user)\n",
    "price_path = \"/user/{}/data/data1/shop_price.csv\".format(user)\n",
    "product_join_price_path = \"/user/{}/data/pyspark2\".format(user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceProductDF = (spark.read\n",
    "                   .option(\"header\", \"false\")\n",
    "                   .option(\"sep\", \"\\t\")\n",
    "                   .csv(product_path)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceProductDF.printSchema()\n",
    "sourceProductDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcePriceDF = (spark.read\n",
    "                 .option(\"header\", \"false\")\n",
    "                 .option(\"sep\", \";\")\n",
    "                 .csv(price_path)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcePriceDF.printSchema()\n",
    "sourcePriceDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приводим типы и задаем названия полей.\n",
    "productDF = (\n",
    "    sourceProductDF\n",
    "    .select(\n",
    "        sf.col(\"_c0\").cast(IntegerType()).alias(\"product_id\"),\n",
    "        sf.col(\"_c1\").alias(\"description\")\n",
    "    )\n",
    ")\n",
    "\n",
    "priceDF = (\n",
    "    sourcePriceDF\n",
    "    .select(\n",
    "        sf.col(\"_c0\").cast(IntegerType()).alias(\"product_id\"),\n",
    "        sf.col(\"_c1\").cast(DoubleType()).alias(\"price\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "productDF.printSchema()\n",
    "productDF.show()\n",
    "\n",
    "priceDF.printSchema()\n",
    "priceDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем inner-join\n",
    "resultDF = (\n",
    "    productDF\n",
    "    .join(priceDF, productDF.product_id == priceDF.product_id, how='inner')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Спарк сам по себе ничего не запоминает, \n",
    "# в примере ниже для расчета 2 раза используется count, \n",
    "# все расчеты будут выполнены 2 раза.\n",
    "print \"1, Count: {cnt}\".format(cnt=resultDF.count())\n",
    "print \"2, Count/2: {cnt}\".format(cnt=resultDF.count()/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Правильно было бы записать результат в переменную и использовать ее\n",
    "# Это позволит избежать 2-х расчетов count\n",
    "cnt = resultDF.count()\n",
    "print \"1, Count: {cnt}\".format(cnt=cnt)\n",
    "print \"2, Count/2: {cnt}\".format(cnt=cnt/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.show()\n",
    "cnt = resultDF.count()\n",
    "print \"Count: {cnt}\".format(cnt=cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Делаем inner-join и убираем дублирующую колонку\n",
    "resultDF = (\n",
    "    productDF\n",
    "    .join(priceDF, productDF.product_id == priceDF.product_id, how='inner')\n",
    "    .select(\n",
    "        productDF.product_id,\n",
    "        sf.col(\"description\"),\n",
    "        sf.col(\"price\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.show()\n",
    "cnt = resultDF.count()\n",
    "print \"Count: {cnt}\".format(cnt=cnt)\n",
    "\n",
    "\n",
    "# Для примера выведем строки с product_id равным 3 и 7.\n",
    "(\n",
    "    resultDF\n",
    "    .where((sf.col(\"product_id\") == 3) | (sf.col(\"product_id\") == 7))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем результат. mode(\"overwrite\") позволяет перезаписывать результат.\n",
    "# Стоит обратить внимание, что мы сначала скидываем все данные в одну партицию (repartition) и только затем сортируем.\n",
    "(resultDF\n",
    " .repartition(1)\n",
    " .sortWithinPartitions(sf.col(\"price\").desc())\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"sep\", \"\\t\")\n",
    " .csv(product_join_price_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# После всех экспериментов собираем итоговый \"чистый\" код:\n",
    "sourceProductDF = (spark.read\n",
    "                   .option(\"header\", \"false\")\n",
    "                   .option(\"sep\", \"\\t\")\n",
    "                   .csv(product_path)\n",
    "                  )\n",
    "\n",
    "sourcePriceDF = (spark.read\n",
    "                 .option(\"header\", \"false\")\n",
    "                 .option(\"sep\", \";\")\n",
    "                 .csv(price_path)\n",
    "                )\n",
    "\n",
    "productDF = (\n",
    "    sourceProductDF\n",
    "    .select(\n",
    "        sf.col(\"_c0\").alias(\"product_id\"),\n",
    "        sf.col(\"_c1\").alias(\"description\")\n",
    "    )\n",
    ")\n",
    "\n",
    "priceDF = (\n",
    "    sourcePriceDF\n",
    "    .select(\n",
    "        sf.col(\"_c0\").alias(\"product_id\"),\n",
    "        sf.col(\"_c1\").cast(DoubleType()).alias(\"price\")\n",
    "    )\n",
    ")\n",
    "\n",
    "resultDF = (\n",
    "    productDF\n",
    "    .join(priceDF, productDF.product_id == priceDF.product_id, how='inner')\n",
    "    .select(\n",
    "        productDF.product_id,\n",
    "        sf.col(\"description\"),\n",
    "        sf.col(\"price\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "(resultDF\n",
    " .repartition(1)\n",
    " .sortWithinPartitions(sf.col(\"price\").asc())\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"sep\", \"\\t\")\n",
    " .csv(product_join_price_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# После работы обязательно отключаем спарк и отдаем ресурсы!\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
