{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyspark3\n",
    "Сделать WordCount и TopN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"HADOOP_CONF_DIR\"]=\"/etc/hadoop/conf\"\n",
    "# os.environ[\"SPARK_HOME\"]=\"/usr/hdp/current/spark2-client\"\n",
    "# os.environ[\"JAVA_HOME\"]=\"/usr/java/jdk1.8.0_191/jre\"\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"yarn\")\\\n",
    "    .appName(\"v_alehin_pyspark3\")\\\n",
    "    .config(\"spark.executor.instances\", \"1\")\\\n",
    "    .config(\"spark.executor.memory\", \"1G\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")\\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"300s\")\\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"1000\")\\\n",
    "    .config(\"spark.driver.memory\", \"1G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"1G\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark_context = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Urls:\n",
    "Yarn       http://91.219.226.252:8088/cluster/scheduler\n",
    "Spark      http://91.219.226.252:8088/proxy/{app_id}/stages/\n",
    "App info   http://91.219.226.252:8088/cluster/app/{app_id}/\n",
    "\"\"\".format(app_id=spark_context.applicationId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "topn = 10\n",
    "\n",
    "user = \"v.alehin\"  # your login\n",
    "\n",
    "#raw_text_path = \"/user/{}/data/data1/wctest.txt\".format(user)\n",
    "raw_text_path = \"/user/{}/data1/books/*\".format(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsDF = spark.read.text(raw_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsDF.printSchema()\n",
    "docsDF.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountDF = (\n",
    "    docsDF\n",
    "    .select(\n",
    "        sf.explode(\n",
    "            sf.split(\n",
    "                sf.regexp_replace(\n",
    "                    sf.lower(sf.col(\"value\")), \n",
    "                    \"([^a-z ]+)\",\n",
    "                    \"\"),\n",
    "                \" \")\n",
    "        ).alias(\"word\"))\n",
    "    .where(sf.col(\"word\") != \"\")\n",
    "    .groupBy(sf.col(\"word\"))\n",
    "    .agg(sf.count(\"word\").alias(\"cnt\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountDF.printSchema()\n",
    "wordCountDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topNDF = (\n",
    "    wordCountDF\n",
    "    .orderBy(sf.col(\"cnt\").desc())\n",
    "    .limit(topn)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topNDF.printSchema()\n",
    "topNDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# После всех экспериментов собираем итоговый \"чистый\" код:\n",
    "docsDF = spark.read.text(raw_text_path)\n",
    "\n",
    "wordCountDF = (\n",
    "    docsDF\n",
    "    .select(\n",
    "        sf.explode(\n",
    "            sf.split(\n",
    "                sf.regexp_replace(\n",
    "                    sf.lower(sf.col(\"value\")), \n",
    "                    \"([^a-z ]+)\",\n",
    "                    \"\"),\n",
    "                \" \")\n",
    "        ).alias(\"word\"))\n",
    "    .where(sf.col(\"word\") != \"\")\n",
    "    .groupBy(sf.col(\"word\"))\n",
    "    .agg(sf.count(\"word\").alias(\"cnt\"))\n",
    ")\n",
    "\n",
    "topNDF = (\n",
    "    wordCountDF\n",
    "    .orderBy(sf.col(\"cnt\").desc())\n",
    "    .limit(topn)\n",
    ")\n",
    "\n",
    "topNDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# После работы обязательно отключаем спарк и отдаем ресурсы!\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
