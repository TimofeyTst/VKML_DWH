Домашнее задание #4

Дедлайн: 2024-04-28 23:59:59 
Дедлайн 2 со штрафом 10% от итоговых баллов: 2024-04-23 23:59:59
Решения после 2024-04-23 23:59:59 приниматься не будут.

 
На основе данных Федеральной Службы Государственной Статистики была сделана выборка средних потребительских цен (тарифов) на товары и услуги по некоторым городам России.
Цены указаны в рублях.

Описание сырых данных (rosstat в data3.zip):
1. city.csv - справочник городов (city[string], city_id[int])
2. product.csv - справочник товаров и услуг (product[string], product_id[int])
3. /price - данные по ценам на товары в разных городах (city_id[int], product_id[int], price[float])
4. products_for_stat.csv - идентификаторы товаров для которых нужно собрать статистику (product_id[int]).

Описание сырых данных (ok в hadoop3.zip).
1. coreDemography - данные демографии пользователей (user_id, create_date, birth_date (число дней с 1970-01-01), gender, id_country, id_city, login_region)
2. Справочник стран geography/countries.csv (name,id)
3. Справочник соответствия городов ОК - Росстат geography/rs_city.csv (ok_city_id, rs_city_id)

Часть 1: реализация Spark-приложений для вычисления статистики.

Необходимо собрать статистику для товаров из products_for_stat.csv (и только для товаров из этого файла!).
Для предложенных товаров необходимо вычислить минимальную, максимальную и среднюю цену по всем городам (будет получен датасет price_stat), результат сохранить в hdfs. 
Далее, из набора данных ОК следует для всех пользователей из городов, цена на товары в которых выше средней, собрать статистику, которая будет содержать: название города, число пользователей из этого города, средний возраст пользователей, число пользователей мужчин, число пользователей женщин, доля мужчин, доля женщин (датасет ok_dem). 
Из полученного датасета нужно будет выбрать города с максимальным и минимальным средним возрастом, максимальной долей мужчин и максимальной долей женщин. 
Для этих городов в данных Росстат нужно будет выбрать самый дешевый и самый дорогой товары, разницу в цене между ними (датасет product_stat)

Итого в hdfs вы должны получить 3 директории с результатами шагов: price_stat, ok_dem, product_stat, со следующими полями:
price_stat
- product_id
- min_price
- max_price
- price_avg

ok_dem - cохранить результат нужно в один файл с сортировкой по убыванию user_cnt.
- city_name
- user_cnt
- age_avg
- men_cnt
- women_cnt
- men_share
- women_share

product_stat
- city_name
- cheapest_product_name
- most_expensive_product_name
- price_difference

Требования к решению:
-При желании можно реализовать шаги на MapReduce, но как минимум один из шагов должен быть раелизован на Spark
-При вычислении возраста пользователей будем вычислять его для фиксированной даты current_dt. Параметр current_dt будем считать равным 2023-03-01
-Разделитель в итоговых файлах с данными - точка с запятой (;)
-Все вещественные числа должны быть округлены до 2 знаков после запятой.
-Результаты пишем в директорию на hdfs /user/$USER/task3. Если вам потребуется сохранять промежуточные результаты или предподготовленные наборы данных предлагается сохранять их в поддиректории /user/$USER/task3/stage с осмысленным названием.


Преимущественно будут влиять на итоговую оценку по этой задаче:
-Соответствие решения поставленной задаче.
-"Чистота" кода. Код следует оформлять аккуратно, давая понятные названия переменным, соблюдая отступы, в нетривиальных местах следует писать комментарии.
-Оптимальность решения (использование памяти, фильтрация ненужных данных, экономия памяти на стадии shuffle, использование подходящих типов/структур данных). 

Советы/рекомендации по решению:
-Для начала подготовьте маленькую выборку данных и прогоните на ней свое решение.
-При выполнении задания следует подумать над порядком действий (сначала фильтрация-потом джоин; сначала группировка-потом джоин)
-При решении следует думать о том, что данных по ценам и пользователям может быть очень и очень много.
-Посмотрите на сырые данные, возможно что-то можно отфильтровать.
-Разберитесь с механизмом Broadcast Hash Join и Distributed Cache (-file).

Часть 2: ETL
Во второй части вам необходимо реализованные шаги в первой части объединить в единый пайплайн используя фреймворк Luigi

Требования к решению:
- Финальным результатом считаем датасет в директории product_stat. При наличии данных в директории процесс должен завершаться успешно, даже при отсутствии данных других шагов (не должны выполняться никакие вычисления, считаем что работа уже выполнена).
- При отсутствии финальных результатов, вычисления должны запускаться только для шагов, результаты которых отсутствуют
- Опционально можно сделать параметр current_dt параметром, передаваемым при запуске

Если вы выполняете только часть 1, допускается сдавать скрипты в виде Jupyter-ноутбуков
